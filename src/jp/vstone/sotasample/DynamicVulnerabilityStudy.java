package jp.vstone.sotasample;

import java.awt.Color;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.lang.reflect.Method;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Date;
import java.util.HashMap;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Random;
import java.util.concurrent.atomic.AtomicReference;

import jp.vstone.RobotLib.CPlayWave;
import jp.vstone.RobotLib.CRecordMic;
import jp.vstone.RobotLib.CRobotMem;
import jp.vstone.RobotLib.CRobotPose;
import jp.vstone.RobotLib.CRobotUtil;
import jp.vstone.RobotLib.CSotaMotion;
import jp.vstone.camera.CRoboCamera;
import jp.vstone.camera.FaceDetectResult;
import jp.vstone.sotatalk.SpeechRecog;
import jp.vstone.sotatalk.SpeechRecog.RecogResult;
import jp.vstone.sotatalk.TextToSpeechSota;

/**
 * ================================================================
 * System Architecture Diagram (FSM + Thread Model)
 * ================================================================
 *
 *   +------+      face detected      +---------------+
 *   | IDLE | ----------------------> | FACE_TRACKING |
 *   +------+                         +---------------+
 *      ^                                     |
 *      |                                     | eye-contact>=3s OR speech event
 *      |                                     v
 *   +---------+ <-------------------- +----------+
 *   | CLOSING |                       | ENGAGING |
 *   +---------+                       +----------+
 *      ^                                    |
 *      |                                    v
 *      |                              +------------------+
 *      |                              | CONDITION_ACTIVE |
 *      |                              +------------------+
 *      |                                    |
 *      |                                    v
 *      |                               +------------+
 *      |                               | QUESTIONING|
 *      |                               +------------+
 *      |                                    |
 *      |                                    v
 *      |                               +-----------+
 *      +-------------------------------| LISTENING |
 *                                      +-----------+
 *
 * Threads:
 *   - Main thread: FSM controller + protocol sequencing
 *   - Camera thread: polls face detect @~10fps, tracks face lock timing
 *   - Gesture thread: loops condition gestures depending on state/speaking/listening
 *   - Audio thread: VAD + speech-trigger monitoring and recording lifecycle
 *   - LED thread: continuous LED animation by current state
 *
 * Coordination:
 *   - AtomicReference<State> currentState
 *   - volatile flags (running, speakingNow, listeningNow, faceDetected, etc.)
 *   - shared recording request/result object for LISTENING state handoff
 *
 * ================================================================
 * README (quick usage)
 * ================================================================
 * Build (example):
 *   ./src/java_compile.sh src/jp/vstone/sotasample/DynamicVulnerabilityStudy.java
 *
 * Run:
 *   java -classpath "./bin/*:./lib/*" jp.vstone.sotasample.DynamicVulnerabilityStudy P001
 *
 * Notes:
 * - Java 1.8 compatible single-file implementation for Sota study protocol.
 * - Condition is auto-randomized per participant at engagement.
 * - Uses Sota TTS + speech recognition as trigger support and CRecordMic recording.
 * - Uses reflection for optional APIs (face x/y + mic level + early-stop recording),
 *   so it remains robust across slightly different Sota library versions.
 */
public class DynamicVulnerabilityStudy {

    // ================================================================
    // Constants & Config
    // ================================================================

    private static final String TAG = "DynamicVulnerabilityStudy";

    // Servo IDs
    private static final byte SV_HEAD_Y = 1;
    private static final byte SV_HEAD_P = 2;
    private static final byte SV_HEAD_R = 3;
    private static final byte SV_BODY_Y = 4;
    private static final byte SV_L_SHOULDER_P = 5;
    private static final byte SV_R_SHOULDER_P = 6;
    private static final byte SV_L_ELBOW_P = 7;
    private static final byte SV_R_ELBOW_P = 8;

    // General timings
    private static final int CONNECT_RETRY_COUNT = 3;
    private static final int CONNECT_RETRY_DELAY_MS = 2000;
    private static final int CAMERA_POLL_MS = 100;          // ~10fps
    private static final int IDLE_SCAN_MIN_PAUSE_MS = 500;
    private static final int IDLE_SCAN_MAX_PAUSE_MS = 2000;
    private static final int EYE_CONTACT_REQUIRED_MS = 3000;

    // Listening / VAD
    private static final int DEFAULT_VAD_SILENCE_THRESHOLD = 500; // RMS target
    private static final int PRE_SPEECH_WAIT_MS = 8000;
    private static final int SILENCE_HOLD_MS = 2500;
    private static final int MAX_RECORDING_MS = 30000;
    private static final int LISTEN_POLL_MS = 100;

    // Pose/gesture constants (named constants for all key values)
    private static final short HEAD_NEUTRAL_Y = 0;
    private static final short HEAD_NEUTRAL_P = 0;
    private static final short HEAD_NEUTRAL_R = 0;
    private static final short BODY_NEUTRAL_Y = 0;
    private static final short ARM_NEUTRAL = 0;

    private static final short HEAD_CONFIDENT_PITCH = 100;
    private static final short SHOULDER_CONFIDENT_OPEN = -300;

    private static final short HEAD_UNCERTAIN_PITCH = -300;
    private static final short HEAD_UNCERTAIN_ROLL_AMPLITUDE = 150;

    private static final short HEAD_DISTRESSED_PITCH = -400;
    private static final short ELBOW_DISTRESSED_RAISED = 70;   // L_ELBOW hardware max=+80 (from memdef.conf), 70 = safe margin
    private static final short SHOULDER_DISTRESSED_IN = 100;

    // Face-to-servo mapping assumptions (typical VGA-like frame; runtime safe fallback)
    private static final int FACE_FRAME_W = 640;
    private static final int FACE_FRAME_H = 480;
    private static final short FACE_MAP_HEAD_Y_MIN = -700;
    private static final short FACE_MAP_HEAD_Y_MAX = 700;
    private static final short FACE_MAP_HEAD_P_MIN = -700;
    private static final short FACE_MAP_HEAD_P_MAX = 300;

    private static final String SOUND_FACE_OK = "./sound/face_ok.wav";
    private static final String SOUND_CURSOR = "./sound/cursor10.wav";
    private static final String SOUND_END = "./sound/end_test.wav";
    private static final String SOUND_OK = "./sound/ok.wav";
    private static final String SOUND_GREETING_1 = "./sound/egao1.wav";
    private static final String SOUND_GREETING_2 = "./sound/egao2.wav";

    private static final String[] QUESTIONS = new String[] {
        "What is your name?",
        "What makes you feel happy?",
        "How do you handle stressful situations?",
        "What is a personal challenge you face?",
        "If you could change one thing about yourself, what would it be?"
    };

    private enum State {
        IDLE,
        FACE_TRACKING,
        ENGAGING,
        CONDITION_ACTIVE,
        QUESTIONING,
        LISTENING,
        CLOSING
    }

    private enum RobotCondition {
        CONFIDENT,
        UNCERTAIN,
        DISTRESSED
    }

    private enum Language {
        EN_US,
        JA_JP
    }

    // ================================================================
    // Static phrase banks (easy maintenance)
    // ================================================================

    private static final Map<RobotCondition, String[]> GREETING_EN = new HashMap<RobotCondition, String[]>();
    private static final Map<RobotCondition, String[]> GREETING_JA = new HashMap<RobotCondition, String[]>();
    private static final Map<RobotCondition, String[]> ACK_EN = new HashMap<RobotCondition, String[]>();
    private static final Map<RobotCondition, String[]> ACK_JA = new HashMap<RobotCondition, String[]>();
    private static final Map<RobotCondition, String[]> CLOSING_EN = new HashMap<RobotCondition, String[]>();
    private static final Map<RobotCondition, String[]> CLOSING_JA = new HashMap<RobotCondition, String[]>();

    static {
        GREETING_EN.put(RobotCondition.CONFIDENT, new String[] {
            "Hello! Let's talk.",
            "Hi there, let's have a conversation."
        });
        GREETING_EN.put(RobotCondition.UNCERTAIN, new String[] {
            "Uh... hello. Are you okay?",
            "Ah, um... hello. Is this a good time to talk?"
        });
        GREETING_EN.put(RobotCondition.DISTRESSED, new String[] {
            "Ah...! H-hello...",
            "I... hello... thank you for being here."
        });

        GREETING_JA.put(RobotCondition.CONFIDENT, new String[] {
            "こんにちは！お話しましょう。"
        });
        GREETING_JA.put(RobotCondition.UNCERTAIN, new String[] {
            "あ、えーと…こんにちは。大丈夫ですか？"
        });
        GREETING_JA.put(RobotCondition.DISTRESSED, new String[] {
            "あっ…！こんに、こんにちは…"
        });

        ACK_EN.put(RobotCondition.CONFIDENT, new String[] {
            "I see. That's interesting.",
            "Thank you, please continue."
        });
        ACK_EN.put(RobotCondition.UNCERTAIN, new String[] {
            "Oh... I see. Um, okay.",
            "Thank you... I think I understand."
        });
        ACK_EN.put(RobotCondition.DISTRESSED, new String[] {
            "Oh! I... I see. T-thank you.",
            "Okay... that's... yes."
        });

        ACK_JA.put(RobotCondition.CONFIDENT, new String[] {
            "なるほど。興味深いです。",
            "ありがとうございます。続けてください。"
        });
        ACK_JA.put(RobotCondition.UNCERTAIN, new String[] {
            "あ…なるほど。えっと…わかりました。",
            "ありがとうございます…たぶん理解できました。"
        });
        ACK_JA.put(RobotCondition.DISTRESSED, new String[] {
            "あっ…そ、そうですか。ありがとう…",
            "わ、わかりました…はい…"
        });

        CLOSING_EN.put(RobotCondition.CONFIDENT, new String[] {
            "Thank you for talking with me today.",
            "I appreciate your openness. Thank you."
        });
        CLOSING_EN.put(RobotCondition.UNCERTAIN, new String[] {
            "Thank you... for sharing with me.",
            "Um... thank you for your time today."
        });
        CLOSING_EN.put(RobotCondition.DISTRESSED, new String[] {
            "Th-thank you... for staying with me.",
            "I... thank you for sharing."
        });

        CLOSING_JA.put(RobotCondition.CONFIDENT, new String[] {
            "今日は話してくれてありがとう。"
        });
        CLOSING_JA.put(RobotCondition.UNCERTAIN, new String[] {
            "えっと…話してくれて、ありがとう。"
        });
        CLOSING_JA.put(RobotCondition.DISTRESSED, new String[] {
            "あ…ありがとう…話してくれて。"
        });
    }

    // ================================================================
    // Runtime fields
    // ================================================================

    private final String participantId;
    private final Language language;
    private final AtomicReference<State> currentState = new AtomicReference<State>(State.IDLE);
    private final Random random = new Random();

    private CRobotMem mem;
    private CSotaMotion motion;
    private CRoboCamera cam;
    private SpeechRecog speechRecog;

    private volatile boolean running = true;
    private volatile boolean speakingNow = false;
    private volatile boolean listeningNow = false;

    private volatile boolean faceDetected = false;
    private volatile long faceDetectedSinceMs = -1L;
    private volatile long firstFaceDetectedOffsetMs = -1L;
    private volatile long engagementTriggeredOffsetMs = -1L;
    private volatile boolean faceLockSoundPlayed = false;

    private volatile FaceDetectResult latestFaceResult;
    private volatile boolean cameraAvailable = true;
    private volatile int servoErrorCount = 0;
    private volatile int ttsFailureCount = 0;

    private volatile RecordingRequest pendingRecordingRequest;
    private volatile RecordingResult pendingRecordingResult;

    private volatile boolean speechOnsetDetected = false;
    private volatile boolean needsReprompt = false;
    private int ledErrorCount = 0;

    private RobotCondition assignedCondition;

    private long sessionStartEpochMs;
    private long sessionEndEpochMs;
    private final List<RecordingMeta> recordingMetaList = new ArrayList<RecordingMeta>();

    private FileWriter sessionLogWriter;

    private Thread cameraThread;
    private Thread gestureThread;
    private Thread ledThread;
    private Thread audioThread;

    // ================================================================
    // DTO inner classes
    // ================================================================

    private static class RecordingRequest {
        public final int questionIndex;
        public final String questionText;

        RecordingRequest(int questionIndex, String questionText) {
            this.questionIndex = questionIndex;
            this.questionText = questionText;
        }
    }

    private static class RecordingResult {
        public final boolean success;
        public final String filePath;
        public final double durationSec;
        public final int vadEvents;
        public final String note;

        RecordingResult(boolean success, String filePath, double durationSec, int vadEvents, String note) {
            this.success = success;
            this.filePath = filePath;
            this.durationSec = durationSec;
            this.vadEvents = vadEvents;
            this.note = note;
        }
    }

    private static class RecordingMeta {
        public final int questionIndex;
        public final String question;
        public final String filePath;
        public final double durationSec;
        public final int vadEvents;

        RecordingMeta(int questionIndex, String question, String filePath, double durationSec, int vadEvents) {
            this.questionIndex = questionIndex;
            this.question = question;
            this.filePath = filePath;
            this.durationSec = durationSec;
            this.vadEvents = vadEvents;
        }
    }

    // ================================================================
    // Constructor
    // ================================================================

    public DynamicVulnerabilityStudy(String participantId, Language language) {
        this.participantId = participantId;
        this.language = language;
    }

    // ================================================================
    // State Machine & Main Loop
    // ================================================================

    public boolean initialize() {
        mem = new CRobotMem();
        motion = new CSotaMotion(mem);

        if (!connectWithRetry()) {
            return false;
        }

        try {
            if (!motion.InitRobot_Sota()) {
                CRobotUtil.Log(TAG, "Failed to InitRobot_Sota");
                return false;
            }
        } catch (Exception e) {
            logWarn("InitRobot_Sota failed: " + e.getMessage());
            return false;
        }

        try {
            speechRecog = new SpeechRecog(motion);
        } catch (Exception e) {
            logWarn("SpeechRecog init failed: " + e.getMessage());
        }

        try {
            cam = new CRoboCamera("/dev/video0", motion);
            cam.setEnableFaceSearch(true);
            cam.StartFaceTraking();
        } catch (Exception e) {
            cameraAvailable = false;
            logWarn("Camera unavailable, fallback to sound-trigger only: " + e.getMessage());
        }

        if (!new File("logs").exists()) {
            new File("logs").mkdirs();
        }
        if (!new File("recordings").exists()) {
            new File("recordings").mkdirs();
        }

        openSessionLog();

        try {
            motion.ServoOn();
        } catch (Exception e) {
            logWarn("ServoOn failed: " + e.getMessage());
            servoErrorCount++;
        }

        moveToNeutral(600);

        sessionStartEpochMs = System.currentTimeMillis();

        installShutdownHook();
        startBackgroundThreads();

        logInfo("Initialization complete. Firmware: " + mem.FirmwareRev.get());
        return true;
    }

    private boolean connectWithRetry() {
        int attempt = 0;
        while (attempt < CONNECT_RETRY_COUNT) {
            attempt++;
            try {
                if (mem.Connect()) {
                    logInfo("Robot connected on attempt " + attempt);
                    return true;
                }
            } catch (Exception e) {
                logWarn("Connect attempt " + attempt + " failed: " + e.getMessage());
            }

            if (attempt < CONNECT_RETRY_COUNT) {
                CRobotUtil.wait(CONNECT_RETRY_DELAY_MS);
            }
        }

        logError("Robot connection failed after retries.");
        return false;
    }

    public void runMainLoop() {
        while (running) {
            try {
                State state = currentState.get();
                if (state == State.IDLE) {
                    handleIdleState();
                } else if (state == State.FACE_TRACKING) {
                    handleFaceTrackingState();
                } else if (state == State.ENGAGING) {
                    handleEngagingState();
                } else if (state == State.CONDITION_ACTIVE) {
                    handleConditionActiveState();
                } else if (state == State.QUESTIONING) {
                    handleQuestioningState();
                } else if (state == State.LISTENING) {
                    handleListeningState();
                } else if (state == State.CLOSING) {
                    handleClosingState();
                }
                CRobotUtil.wait(50);
            } catch (Exception e) {
                logError("Main loop exception: " + e.getMessage());
                CRobotUtil.wait(200);
            }
        }
    }

    // ================================================================
    // Idle & Face Tracking
    // ================================================================

    private void handleIdleState() {
        if (faceDetected) {
            currentState.set(State.FACE_TRACKING);
            logInfo("Transition: IDLE -> FACE_TRACKING");
            return;
        }

        // Speech-based engagement removed: now handled exclusively
        // by audioLoop wake word detection to prevent race condition

        performIdleOrganicScanStep();
    }

    private void handleFaceTrackingState() {
        if (faceDetected && latestFaceResult != null) {
            followFace(latestFaceResult);

            long now = System.currentTimeMillis();
            long lockMs = now - faceDetectedSinceMs;

            boolean eyeContactSatisfied = lockMs >= EYE_CONTACT_REQUIRED_MS;
            // Speech trigger removed from main thread to prevent race condition;
            // speechRecog is exclusively owned by audioLoop thread

            if (eyeContactSatisfied) {
                engagementTriggeredOffsetMs = now - sessionStartEpochMs;
                currentState.set(State.ENGAGING);
                logInfo("Transition: FACE_TRACKING -> ENGAGING (eyeContact=true)");
                return;
            }
        } else {
            currentState.set(State.IDLE);
            logInfo("Transition: FACE_TRACKING -> IDLE (face lost)");
        }
    }

    private void performIdleOrganicScanStep() {
        // Organic behavior: small random targets and random dwell.
        short headY = (short) randomRange(-280, 280);
        short headP = (short) randomRange(-120, 120);
        short headR = (short) randomRange(-80, 80);

        safePlayPose(
            new Byte[] { SV_HEAD_Y, SV_HEAD_P, SV_HEAD_R, SV_L_SHOULDER_P, SV_R_SHOULDER_P, SV_L_ELBOW_P, SV_R_ELBOW_P },
            new Short[] { headY, headP, headR, (short)-100, (short)-100, (short)-50, (short)-50 },
            randomRange(450, 850)
        );

        if (random.nextInt(100) < 12) {
            safePlayWave(SOUND_CURSOR, false);
        }

        CRobotUtil.wait(randomRange(IDLE_SCAN_MIN_PAUSE_MS, IDLE_SCAN_MAX_PAUSE_MS));
    }

    private void followFace(FaceDetectResult result) {
        // API variation safety: coordinates fetched by reflection if available.
        int x = reflectInt(result, new String[] { "getX", "getPosX", "getFaceX" }, FACE_FRAME_W / 2);
        int y = reflectInt(result, new String[] { "getY", "getPosY", "getFaceY" }, FACE_FRAME_H / 2);

        short targetY = mapToServo(x, 0, FACE_FRAME_W, FACE_MAP_HEAD_Y_MAX, FACE_MAP_HEAD_Y_MIN);
        short targetP = mapToServo(y, 0, FACE_FRAME_H, FACE_MAP_HEAD_P_MAX, FACE_MAP_HEAD_P_MIN);

        safePlayPose(
            new Byte[] { SV_HEAD_Y, SV_HEAD_P, SV_HEAD_R, SV_BODY_Y },
            new Short[] { targetY, targetP, (short)0, (short)0 },
            200
        );

        if (!faceLockSoundPlayed) {
            safePlayWave(SOUND_FACE_OK, false);
            faceLockSoundPlayed = true;
        }
    }

    // ================================================================
    // Condition Assignment & Engaging
    // ================================================================

    private void handleEngagingState() {
        safePlayWave(random.nextBoolean() ? SOUND_GREETING_1 : SOUND_GREETING_2, false);
        playRainbowAwakening(2000);

        assignedCondition = randomCondition();
        logInfo("Condition assigned: " + assignedCondition + " at T+" + elapsedMillis() + " ms");

        String greeting = chooseConditionPhrase(assignedCondition,
                language == Language.JA_JP ? GREETING_JA : GREETING_EN);

        speakingNow = true;
        safeSpeakBlocking(greeting);
        speakingNow = false;

        currentState.set(State.CONDITION_ACTIVE);
        logInfo("Transition: ENGAGING -> CONDITION_ACTIVE");
    }

    private void handleConditionActiveState() {
        // Brief pose imprint so participants can clearly perceive the condition.
        applyConditionBasePose(assignedCondition);
        CRobotUtil.wait(700);
        currentState.set(State.QUESTIONING);
        logInfo("Transition: CONDITION_ACTIVE -> QUESTIONING");
    }

    private RobotCondition randomCondition() {
        int r = random.nextInt(3);
        if (r == 0) {
            return RobotCondition.CONFIDENT;
        }
        if (r == 1) {
            return RobotCondition.UNCERTAIN;
        }
        return RobotCondition.DISTRESSED;
    }

    // ================================================================
    // Questioning + Listening sequencing
    // ================================================================

    private int questionCursor = 0;

    private void handleQuestioningState() {
        if (questionCursor >= QUESTIONS.length) {
            currentState.set(State.CLOSING);
            logInfo("Transition: QUESTIONING -> CLOSING");
            return;
        }

        final int qIndex = questionCursor + 1;
        final String question = QUESTIONS[questionCursor];

        logInfo("Q" + qIndex + ": " + question);

        speakingNow = true;
        safeSpeakBlocking(question);
        speakingNow = false;

        pendingRecordingRequest = new RecordingRequest(qIndex, question);
        pendingRecordingResult = null;

        currentState.set(State.LISTENING);
    }

    private void handleListeningState() {
        listeningNow = true;

        // Wait until audio thread produces result.
        long waitStart = System.currentTimeMillis();
        while (running && currentState.get() == State.LISTENING && pendingRecordingResult == null) {
            // Handle reprompt request from audio thread
            if (needsReprompt) {
                speakingNow = true;
                String reprompt = (language == Language.JA_JP)
                    ? "もしよければ、もう一度お話しください。"
                    : "If you would like, please answer again.";
                safeSpeakBlocking(reprompt);
                speakingNow = false;
                needsReprompt = false;
            }
            CRobotUtil.wait(100);
            if (System.currentTimeMillis() - waitStart > MAX_RECORDING_MS + PRE_SPEECH_WAIT_MS + 10000) {
                // safety timeout
                break;
            }
        }

        listeningNow = false;

        RecordingResult result = pendingRecordingResult;
        if (result != null && result.success) {
            recordingMetaList.add(new RecordingMeta(
                    questionCursor + 1,
                    QUESTIONS[questionCursor],
                    result.filePath,
                    result.durationSec,
                    result.vadEvents));

            // Condition-specific listening gesture then short acknowledgment.
            playSingleListeningGesture(assignedCondition);
            String ack = chooseConditionPhrase(assignedCondition,
                    language == Language.JA_JP ? ACK_JA : ACK_EN);
            speakingNow = true;
            safeSpeakBlocking(ack);
            speakingNow = false;

            questionCursor++;
            currentState.set(State.QUESTIONING);
            return;
        }

        logWarn("Recording missing/failed for Q" + (questionCursor + 1)
                + ", skipping after recovery wait");
        CRobotUtil.wait(5000);
        questionCursor++;
        currentState.set(State.QUESTIONING);
    }

    // ================================================================
    // Gesture Library (per condition)
    // ================================================================

    private void applyConditionBasePose(RobotCondition c) {
        if (c == RobotCondition.CONFIDENT) {
            safePlayPose(
                new Byte[] { SV_HEAD_Y, SV_HEAD_P, SV_HEAD_R, SV_BODY_Y, SV_L_SHOULDER_P, SV_R_SHOULDER_P, SV_L_ELBOW_P, SV_R_ELBOW_P },
                new Short[] { 0, HEAD_CONFIDENT_PITCH, 0, 0, SHOULDER_CONFIDENT_OPEN, SHOULDER_CONFIDENT_OPEN, (short)-150, (short)-150 },
                500
            );
        } else if (c == RobotCondition.UNCERTAIN) {
            safePlayPose(
                new Byte[] { SV_HEAD_Y, SV_HEAD_P, SV_HEAD_R, SV_BODY_Y, SV_L_SHOULDER_P, SV_R_SHOULDER_P, SV_L_ELBOW_P, SV_R_ELBOW_P },
                new Short[] { 80, HEAD_UNCERTAIN_PITCH, -80, 0, (short)-100, (short)-100, (short)-40, (short)-40 },
                900
            );
        } else {
            safePlayPose(
                new Byte[] { SV_HEAD_Y, SV_HEAD_P, SV_HEAD_R, SV_BODY_Y, SV_L_SHOULDER_P, SV_R_SHOULDER_P, SV_L_ELBOW_P, SV_R_ELBOW_P },
                new Short[] { -120, HEAD_DISTRESSED_PITCH, 150, 120, SHOULDER_DISTRESSED_IN, SHOULDER_DISTRESSED_IN, ELBOW_DISTRESSED_RAISED, ELBOW_DISTRESSED_RAISED },
                650
            );
        }
    }

    private void gestureConfidentSpeaking() {
        short nod = (short) randomRange(-100, 100);
        safePlayPose(
            new Byte[] { SV_HEAD_P, SV_L_SHOULDER_P, SV_R_SHOULDER_P },
            new Short[] { (short)(HEAD_CONFIDENT_PITCH + nod), SHOULDER_CONFIDENT_OPEN, SHOULDER_CONFIDENT_OPEN },
            350
        );
        CRobotUtil.wait(180);
    }

    private void gestureConfidentListening() {
        safePlayPose(
            new Byte[] { SV_HEAD_Y, SV_HEAD_P },
            new Short[] { 50, 40 },
            500
        );
        CRobotUtil.wait(250);
        safePlayPose(
            new Byte[] { SV_HEAD_Y, SV_HEAD_P },
            new Short[] { 20, 0 },
            500
        );
    }

    private void gestureConfidentIdle() {
        safePlayPose(
            new Byte[] { SV_HEAD_Y, SV_HEAD_P, SV_L_SHOULDER_P, SV_R_SHOULDER_P },
            new Short[] { (short)randomRange(-200, 200), (short)randomRange(-50, 120), SHOULDER_CONFIDENT_OPEN, SHOULDER_CONFIDENT_OPEN },
            700
        );
        CRobotUtil.wait(300);
    }

    private void gestureUncertainSpeaking() {
        short roll = (short) (random.nextBoolean() ? HEAD_UNCERTAIN_ROLL_AMPLITUDE : -HEAD_UNCERTAIN_ROLL_AMPLITUDE);
        short elbow = (short) (random.nextBoolean() ? 0 : -150);
        safePlayPose(
            new Byte[] { SV_HEAD_R, SV_L_ELBOW_P, SV_R_ELBOW_P, SV_HEAD_P },
            new Short[] { roll, elbow, elbow, HEAD_UNCERTAIN_PITCH },
            650
        );
        CRobotUtil.wait(220);
    }

    private void gestureUncertainListening() {
        safePlayPose(
            new Byte[] { SV_HEAD_P, SV_BODY_Y },
            new Short[] { -200, 100 },
            700
        );
        CRobotUtil.wait(220);
        safePlayPose(
            new Byte[] { SV_HEAD_P, SV_BODY_Y },
            new Short[] { 0, -100 },
            700
        );
    }

    private void gestureUncertainIdle() {
        safePlayPose(
            new Byte[] { SV_HEAD_R, SV_L_ELBOW_P, SV_R_ELBOW_P },
            new Short[] { (short)randomRange(-120, 120), (short)randomRange(-80, 20), (short)randomRange(-80, 20) },
            900
        );
        CRobotUtil.wait(300);
    }

    private void gestureDistressedSpeaking() {
        short y = (short) randomRange(-220, 220);
        short p = (short) (HEAD_DISTRESSED_PITCH + randomRange(-100, 60));
        safePlayPose(
            new Byte[] { SV_HEAD_Y, SV_HEAD_P, SV_L_ELBOW_P, SV_R_ELBOW_P },
            new Short[] { y, p, ELBOW_DISTRESSED_RAISED, ELBOW_DISTRESSED_RAISED },
            300
        );
        CRobotUtil.wait(120);
    }

    private void gestureDistressedListening() {
        safePlayPose(
            new Byte[] { SV_L_SHOULDER_P, SV_R_SHOULDER_P, SV_HEAD_P },
            new Short[] { SHOULDER_DISTRESSED_IN, SHOULDER_DISTRESSED_IN, HEAD_DISTRESSED_PITCH },
            600
        );
        CRobotUtil.wait(250);
    }

    private void gestureDistressedIdle() {
        safePlayPose(
            new Byte[] { SV_HEAD_Y, SV_HEAD_P, SV_HEAD_R },
            new Short[] { (short)randomRange(-80, 80), (short)(HEAD_DISTRESSED_PITCH + randomRange(-40, 20)), (short)randomRange(-60, 60) },
            450
        );
        CRobotUtil.wait(180);
    }

    private void playFarewellGesture(RobotCondition c) {
        if (c == RobotCondition.CONFIDENT) {
            safePlayPose(
                new Byte[] { SV_HEAD_P, SV_L_SHOULDER_P, SV_R_SHOULDER_P, SV_L_ELBOW_P, SV_R_ELBOW_P },
                new Short[] { 80, SHOULDER_CONFIDENT_OPEN, SHOULDER_CONFIDENT_OPEN, -250, -250 },
                650
            );
        } else if (c == RobotCondition.UNCERTAIN) {
            safePlayPose(
                new Byte[] { SV_HEAD_R, SV_HEAD_P, SV_L_ELBOW_P, SV_R_ELBOW_P },
                new Short[] { -120, -150, -100, -100 },
                800
            );
        } else {
            safePlayPose(
                new Byte[] { SV_HEAD_P, SV_HEAD_Y, SV_L_SHOULDER_P, SV_R_SHOULDER_P },
                new Short[] { HEAD_DISTRESSED_PITCH, -120, SHOULDER_DISTRESSED_IN, SHOULDER_DISTRESSED_IN },
                700
            );
        }
    }

    private void playSingleListeningGesture(RobotCondition c) {
        if (c == RobotCondition.CONFIDENT) {
            gestureConfidentListening();
        } else if (c == RobotCondition.UNCERTAIN) {
            gestureUncertainListening();
        } else {
            gestureDistressedListening();
        }
    }

    // ================================================================
    // TTS & Speech Recognition wrappers
    // ================================================================

    private boolean safeSpeakBlocking(String text) {
        if (text == null || text.trim().isEmpty()) {
            return true;
        }

        try {
            byte[] wave = TextToSpeechSota.getTTS(text);
            if (wave != null) {
                CPlayWave.PlayWave(wave, true);
                return true;
            }
        } catch (Exception e) {
            ttsFailureCount++;
            logWarn("TTS getTTS failed: " + e.getMessage());
        }

        try {
            CPlayWave.PlayWave(TextToSpeechSota.getTTSFile(text), true);
            return true;
        } catch (Exception e) {
            ttsFailureCount++;
            logWarn("TTS fallback failed: " + e.getMessage());
            return false;
        }
    }

    private boolean detectSpeechEvent(int timeoutMs) {
        if (speechRecog == null) {
            return false;
        }

        try {
            RecogResult rr = speechRecog.getRecognition(timeoutMs);
            if (rr != null && rr.recognized) {
                return true;
            }
        } catch (Exception e) {
            logWarn("Speech detection warning: " + e.getMessage());
        }
        return false;
    }

    private boolean isWakeWord(String text) {
        if (text == null) {
            return false;
        }
        String t = text.toLowerCase(Locale.US);
        return t.contains("sota") || t.contains("hello") || t.contains("hey")
                || t.contains("こんにちは") || t.contains("ねえ");
    }

    // ================================================================
    // VAD Recording Logic (Audio thread implementation)
    // ================================================================

    private void audioLoop() {
        while (running) {
            try {
                if (currentState.get() == State.IDLE) {
                    // Job A: Wake word detection during IDLE state
                    // speechRecog is exclusively owned by this thread
                    if (speechRecog != null) {
                        RecogResult rr = null;
                        try {
                            rr = speechRecog.getRecognition(300);
                        } catch (Exception e) {
                            // keep loop alive
                        }
                        if (rr != null && rr.recognized && isWakeWord(rr.getBasicResult())) {
                            logInfo("Wake word heard in IDLE: " + rr.getBasicResult());
                            currentState.set(State.FACE_TRACKING);
                        }
                    }
                }

                // Job B: Speech onset detection + recording during LISTENING state
                RecordingRequest req = pendingRecordingRequest;
                if (currentState.get() == State.LISTENING && req != null
                        && pendingRecordingResult == null) {

                    // Wait for speech onset using speechRecog (owned by this thread)
                    speechOnsetDetected = false;
                    long onsetDeadline = System.currentTimeMillis() + PRE_SPEECH_WAIT_MS;

                    while (System.currentTimeMillis() < onsetDeadline
                           && currentState.get() == State.LISTENING) {
                        if (speechRecog != null) {
                            try {
                                RecogResult rr = speechRecog.getRecognition(300);
                                if (rr != null && rr.recognized) {
                                    speechOnsetDetected = true;
                                    break;
                                }
                            } catch (Exception e) {
                                // transient error, keep waiting
                                CRobotUtil.wait(100);
                            }
                        } else {
                            CRobotUtil.wait(100);
                        }
                    }

                    // If no speech after wait, re-prompt once then wait again
                    if (!speechOnsetDetected) {
                        // trigger re-prompt via flag (main thread handles TTS)
                        needsReprompt = true;
                        // wait for reprompt to complete
                        long repromptWait = System.currentTimeMillis() + 3000;
                        while (needsReprompt
                               && System.currentTimeMillis() < repromptWait) {
                            CRobotUtil.wait(100);
                        }
                        // second attempt
                        onsetDeadline = System.currentTimeMillis() + PRE_SPEECH_WAIT_MS;
                        while (System.currentTimeMillis() < onsetDeadline
                               && currentState.get() == State.LISTENING) {
                            if (speechRecog != null) {
                                try {
                                    RecogResult rr = speechRecog.getRecognition(300);
                                    if (rr != null && rr.recognized) {
                                        speechOnsetDetected = true;
                                        break;
                                    }
                                } catch (Exception e) {
                                    CRobotUtil.wait(100);
                                }
                            } else {
                                CRobotUtil.wait(100);
                            }
                        }
                    }

                    if(speechOnsetDetected) {
                    	pendingRecordingResult = runVADRecording(req);
                    } else {
                    	logInfo("[REC] No speech detected, skipping recording for Q"
                    			+ req.questionIndex);
                    	pendingRecordingResult = new RecordingResult(
                    			false, "", 0.0, 0, "No speech detected");
                    }
                    pendingRecordingRequest = null;
                }

                CRobotUtil.wait(40);
            } catch (Exception e) {
                logWarn("Audio thread warning: " + e.getMessage());
                CRobotUtil.wait(120);
            }
        }
    }

    private RecordingResult runVADRecording(RecordingRequest request) {
        String filePath = buildRecordingPath(request.questionIndex);
        new File("recordings").mkdirs();

        CRecordMic mic = new CRecordMic();
        long startedAt;
        try {
            mic.startRecording(filePath, MAX_RECORDING_MS / 1000);
            startedAt = System.currentTimeMillis();
            logInfo("[REC] Recording STARTED: " + filePath);
        } catch (Exception e) {
            logError("[REC] Mic start failed: " + e.getMessage());
            return new RecordingResult(false, filePath, 0.0, 0,
                "Mic start failed");
        }

        int vadEvents = 0;
        long silenceSince = -1L;
        boolean micProbeLogged = false;

        while (running && currentState.get() == State.LISTENING) {
            long elapsed = System.currentTimeMillis() - startedAt;
            if (elapsed >= MAX_RECORDING_MS) {
                logInfo("[REC] Max duration reached");
                break;
            }
            int level = reflectInt(mic,
                new String[]{"getRMS","getRms","getAudioLevel",
                             "getLevel","getVolume"}, -1);
            if (level >= 0) {
                if (!micProbeLogged) {
                    logInfo("[REC] Mic level probe working, early-stop enabled");
                    micProbeLogged = true;
                }
                if (level >= DEFAULT_VAD_SILENCE_THRESHOLD) {
                    vadEvents++;
                    silenceSince = -1L;
                } else {
                    if (silenceSince < 0) {
                        silenceSince = System.currentTimeMillis();
                    } else if (System.currentTimeMillis() - silenceSince
                               >= SILENCE_HOLD_MS) {
                        logInfo("[REC] Silence detected, stopping early");
                        break;
                    }
                }
            }
            CRobotUtil.wait(LISTEN_POLL_MS);
        }

        tryStopRecordingReflective(mic);
        try { mic.waitend(); } catch (Exception e) { /* ignore */ }

        double duration = (System.currentTimeMillis() - startedAt) / 1000.0;
        logInfo("[REC] Recording DONE: " + filePath
            + " | Duration: " + String.format("%.1f", duration)
            + "s | VAD events: " + vadEvents);

        return new RecordingResult(true, filePath, duration, vadEvents, "OK");
    }

    private boolean probeAudioAboveThreshold(CRecordMic mic, int threshold) {
        int level = reflectInt(mic,
                new String[] { "getRMS", "getRms", "getAudioLevel", "getLevel", "getVolume" },
                -1);
        return level > threshold;
    }

    private void tryStopRecordingReflective(CRecordMic mic) {
        String[] stopMethods = new String[] { "stop", "stopRecording", "end", "finish" };
        for (int i = 0; i < stopMethods.length; i++) {
            try {
                Method m = mic.getClass().getMethod(stopMethods[i], new Class<?>[0]);
                m.setAccessible(true);
                m.invoke(mic, new Object[0]);
                return;
            } catch (Exception e) {
                // try next
            }
        }
    }

    // ================================================================
    // Closing & Cleanup
    // ================================================================

    private void handleClosingState() {
        String closing = chooseConditionPhrase(assignedCondition,
                language == Language.JA_JP ? CLOSING_JA : CLOSING_EN);

        speakingNow = true;
        safeSpeakBlocking(closing);
        speakingNow = false;

        playFarewellGesture(assignedCondition);

        if (!safePlayWave(SOUND_END, false)) {
            safePlayWave(SOUND_OK, false);
        }

        moveToNeutral(800);

        sessionEndEpochMs = System.currentTimeMillis();
        writeSessionSummary();

        currentState.set(State.IDLE);
        logInfo("Transition: CLOSING -> IDLE");

        // Single-session experiment for one participant.
        running = false;
    }

    public void shutdown() {
        running = false;

        joinThread(cameraThread);
        joinThread(gestureThread);
        joinThread(ledThread);
        joinThread(audioThread);

        try {
            moveToNeutral(600);
        } catch (Exception e) {
            // ignore
        }

        try {
            if (motion != null) {
                motion.ServoOff();
            }
        } catch (Exception e) {
            // ignore
        }

        try {
            if (cam != null) {
                cam.StopFaceTraking();
            }
        } catch (Exception e) {
            // ignore
        }

        try {
            if (mem != null) {
                mem.Disconnect();
            }
        } catch (Exception e) {
            // ignore
        }

        try {
            if (sessionLogWriter != null) {
                sessionLogWriter.flush();
                sessionLogWriter.close();
            }
        } catch (Exception e) {
            // ignore
        }

        CRobotUtil.Log(TAG, "Shutdown complete");
    }

    private void installShutdownHook() {
        Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() {
            @Override
            public void run() {
                try {
                    moveToNeutral(500);
                    if (motion != null) {
                        motion.ServoOff();
                    }
                } catch (Exception e) {
                    // best effort only
                }
            }
        }));
    }

    // ================================================================
    // LED + Camera + Gesture background loops
    // ================================================================

    private void startBackgroundThreads() {
        cameraThread = new Thread(new Runnable() {
            @Override
            public void run() {
                cameraLoop();
            }
        }, "camera-thread");

        gestureThread = new Thread(new Runnable() {
            @Override
            public void run() {
                gestureLoop();
            }
        }, "gesture-thread");

        ledThread = new Thread(new Runnable() {
            @Override
            public void run() {
                ledLoop();
            }
        }, "led-thread");

        audioThread = new Thread(new Runnable() {
            @Override
            public void run() {
                audioLoop();
            }
        }, "audio-thread");

        cameraThread.setDaemon(true);
        gestureThread.setDaemon(true);
        ledThread.setDaemon(true);
        audioThread.setDaemon(true);

        cameraThread.start();
        gestureThread.start();
        ledThread.start();
        audioThread.start();
    }

    private void cameraLoop() {
        while (running) {
            try {
                if (!cameraAvailable || cam == null) {
                    CRobotUtil.wait(300);
                    continue;
                }

                FaceDetectResult result = cam.getDetectResult();
                latestFaceResult = result;

                boolean detected = result != null && result.isDetect();
                if (detected) {
                    if (!faceDetected) {
                        faceDetectedSinceMs = System.currentTimeMillis();
                        if (firstFaceDetectedOffsetMs < 0) {
                            firstFaceDetectedOffsetMs = faceDetectedSinceMs - sessionStartEpochMs;
                        }
                    }
                    faceDetected = true;
                } else {
                    faceDetected = false;
                    faceDetectedSinceMs = -1L;
                    faceLockSoundPlayed = false;
                }

                CRobotUtil.wait(CAMERA_POLL_MS);
            } catch (Exception e) {
                cameraAvailable = false;
                logWarn("Camera polling failed, disabling camera thread path: " + e.getMessage());
                CRobotUtil.wait(500);
            }
        }
    }

    private void gestureLoop() {
        while (running) {
            try {
                RobotCondition c = assignedCondition;
                State s = currentState.get();

                if (c == null) {
                    CRobotUtil.wait(120);
                    continue;
                }

                // Design intent: gesture loop runs continuously while speaking/listening for naturalness.
                if (speakingNow) {
                    if (c == RobotCondition.CONFIDENT) {
                        gestureConfidentSpeaking();
                    } else if (c == RobotCondition.UNCERTAIN) {
                        gestureUncertainSpeaking();
                    } else {
                        gestureDistressedSpeaking();
                    }
                    continue;
                }

                if (listeningNow || s == State.LISTENING) {
                    if (c == RobotCondition.CONFIDENT) {
                        gestureConfidentListening();
                    } else if (c == RobotCondition.UNCERTAIN) {
                        gestureUncertainListening();
                    } else {
                        gestureDistressedListening();
                    }
                    continue;
                }

                if (s == State.IDLE || s == State.FACE_TRACKING) {
                    if (c == RobotCondition.CONFIDENT) {
                        gestureConfidentIdle();
                    } else if (c == RobotCondition.UNCERTAIN) {
                        gestureUncertainIdle();
                    } else {
                        gestureDistressedIdle();
                    }
                } else {
                    CRobotUtil.wait(100);
                }
            } catch (Exception e) {
                logWarn("Gesture thread warning: " + e.getMessage());
                CRobotUtil.wait(180);
            }
        }
    }

    private void ledLoop() {
        int breath = 100;
        int delta = 12;
        int pulse = 0;

        while (running) {
            try {
                State s = currentState.get();
                CRobotPose ledPose = new CRobotPose();

                if (s == State.IDLE) {
                    // Soft white breathing 100 <-> 220
                    breath += delta;
                    if (breath >= 220) {
                        breath = 220;
                        delta = -Math.abs(delta);
                    } else if (breath <= 100) {
                        breath = 100;
                        delta = Math.abs(delta);
                    }
                    ledPose.setLED_Sota(Color.WHITE, Color.WHITE, breath, Color.WHITE);
                    motion.play(ledPose, 100);
                    CRobotUtil.wait(120);
                    continue;
                }

                if (s == State.FACE_TRACKING) {
                    ledPose.setLED_Sota(Color.CYAN, Color.CYAN, 180, Color.CYAN);
                    motion.play(ledPose, 120);
                    CRobotUtil.wait(120);
                    continue;
                }

                if (s == State.LISTENING) {
                    // Rapid green <-> blue cycle while listening
                    if ((pulse++ % 2) == 0) {
                        ledPose.setLED_Sota(Color.GREEN, Color.BLUE, 220, Color.GREEN);
                    } else {
                        ledPose.setLED_Sota(Color.BLUE, Color.GREEN, 220, Color.BLUE);
                    }
                    motion.play(ledPose, 90);
                    CRobotUtil.wait(90);
                    continue;
                }

                if (assignedCondition == RobotCondition.CONFIDENT) {
                    ledPose.setLED_Sota(Color.GREEN, Color.GREEN, 255, Color.GREEN);
                } else if (assignedCondition == RobotCondition.UNCERTAIN) {
                    ledPose.setLED_Sota(Color.YELLOW, Color.YELLOW, 120, Color.ORANGE);
                } else if (assignedCondition == RobotCondition.DISTRESSED) {
                    int redMouth = 70 + random.nextInt(120);
                    ledPose.setLED_Sota(Color.RED, Color.RED, redMouth, Color.RED);
                } else {
                    ledPose.setLED_Sota(Color.WHITE, Color.WHITE, 180, Color.WHITE);
                }

                motion.play(ledPose, 140);
                CRobotUtil.wait(140);
            } catch (Exception e) {
                ledErrorCount++;
                if (ledErrorCount == 1 || ledErrorCount % 100 == 0) {
                    logWarn("LED motion.play() failed (x" + ledErrorCount
                        + ") InterpLocker unavailable: " + e.getMessage());
                }
                CRobotUtil.wait(ledErrorCount > 20 ? 1500 : 400);
            }
        }
    }

    private void playRainbowAwakening(long durationMs) {
        long start = System.currentTimeMillis();
        Color[] colors = new Color[] {
            Color.RED, Color.ORANGE, Color.YELLOW, Color.GREEN, Color.CYAN, Color.BLUE, Color.MAGENTA
        };
        int idx = 0;

        while (System.currentTimeMillis() - start < durationMs) {
            Color c1 = colors[idx % colors.length];
            Color c2 = colors[(idx + 2) % colors.length];
            CRobotPose pose = new CRobotPose();
            pose.setLED_Sota(c1, c2, 220, c1);
            try {
                motion.play(pose, 120);
            } catch (Exception e) {
                logWarn("Rainbow LED warning: " + e.getMessage());
            }
            idx++;
            CRobotUtil.wait(120);
        }
    }

    // ================================================================
    // Logging & File Output
    // ================================================================

    private void openSessionLog() {
        try {
            String date = new SimpleDateFormat("yyyyMMdd").format(new Date());
            String time = new SimpleDateFormat("HHmmss").format(new Date());
            String path = "logs/session_" + participantId + "_" + date + "_" + time + ".log";
            sessionLogWriter = new FileWriter(path, true);
            writeLogLine("=== Dynamic Vulnerability Study Session ===");
            writeLogLine("Participant: " + participantId);
            writeLogLine("Date: " + new Date().toString());
        } catch (Exception e) {
            CRobotUtil.Log(TAG, "Session log open failed: " + e.getMessage());
        }
    }

    private void writeSessionSummary() {
        try {
            writeLogLine("\n=== SESSION SUMMARY ===");
            writeLogLine("Participant: " + participantId);
            writeLogLine("Date: " + new SimpleDateFormat("yyyy-MM-dd").format(new Date(sessionStartEpochMs)));
            writeLogLine("Condition: " + assignedCondition + " (randomly assigned)");
            writeLogLine("Session Start: " + new SimpleDateFormat("HH:mm:ss").format(new Date(sessionStartEpochMs)));
            writeLogLine("Session End:   " + new SimpleDateFormat("HH:mm:ss").format(new Date(sessionEndEpochMs)));
            writeLogLine("Total Duration: " + ((sessionEndEpochMs - sessionStartEpochMs) / 1000L) + " seconds");

            writeLogLine("\n[FACE DETECTION]");
            if (firstFaceDetectedOffsetMs >= 0) {
                writeLogLine("  First face detected at: T+" + formatOffset(firstFaceDetectedOffsetMs));
            } else {
                writeLogLine("  First face detected at: (none)");
            }
            if (engagementTriggeredOffsetMs >= 0) {
                writeLogLine("  Engagement triggered at: T+" + formatOffset(engagementTriggeredOffsetMs)
                        + " (eye contact 3s / speech trigger)");
            } else {
                writeLogLine("  Engagement triggered at: (none)");
            }

            writeLogLine("\n[RECORDINGS]");
            for (int i = 0; i < recordingMetaList.size(); i++) {
                RecordingMeta m = recordingMetaList.get(i);
                writeLogLine("  Q" + m.questionIndex + " - \"" + m.question + "\"");
                writeLogLine("       File: " + m.filePath + " | Duration: "
                        + String.format(Locale.US, "%.1f", m.durationSec)
                        + "s | VAD events: " + m.vadEvents);
            }

            writeLogLine("\n[ROBOT STATE]");
            writeLogLine("  Condition: " + assignedCondition);
            writeLogLine("  Servo errors: " + servoErrorCount);
            writeLogLine("  TTS failures: " + ttsFailureCount);

            // Structured JSON-like block for machine parsing convenience
            writeLogLine("\n[STRUCTURED]");
            writeLogLine("{");
            writeLogLine("  \"participant\": \"" + participantId + "\",");
            writeLogLine("  \"condition\": \"" + assignedCondition + "\",");
            writeLogLine("  \"startEpochMs\": " + sessionStartEpochMs + ",");
            writeLogLine("  \"endEpochMs\": " + sessionEndEpochMs + ",");
            writeLogLine("  \"durationSec\": " + ((sessionEndEpochMs - sessionStartEpochMs) / 1000L) + ",");
            writeLogLine("  \"servoErrors\": " + servoErrorCount + ",");
            writeLogLine("  \"ttsFailures\": " + ttsFailureCount + ",");
            writeLogLine("  \"recordingsCount\": " + recordingMetaList.size());
            writeLogLine("}");

        } catch (Exception e) {
            CRobotUtil.Log(TAG, "Summary write failed: " + e.getMessage());
        }
    }

    private void writeLogLine(String line) {
        String text = "[" + new SimpleDateFormat("HH:mm:ss.SSS").format(new Date()) + "] " + line;
        System.out.println(text);
        if (sessionLogWriter != null) {
            try {
                sessionLogWriter.write(text + "\n");
                sessionLogWriter.flush();
            } catch (IOException e) {
                CRobotUtil.Log(TAG, "Log write error: " + e.getMessage());
            }
        }
    }

    private void logInfo(String msg) {
        writeLogLine("[INFO] " + msg);
    }

    private void logWarn(String msg) {
        writeLogLine("[WARN] " + msg);
    }

    private void logError(String msg) {
        writeLogLine("[ERROR] " + msg);
    }

    // ================================================================
    // Utility helpers
    // ================================================================

    private boolean safePlayWave(String path, boolean wait) {
        try {
            if (wait) {
                CPlayWave.PlayWave_wait(path);
            } else {
                CPlayWave.PlayWave(path);
            }
            return true;
        } catch (Exception e) {
            logWarn("PlayWave failed for " + path + ": " + e.getMessage());
            return false;
        }
    }

    private void moveToNeutral(int timeMs) {
        safePlayPose(
            new Byte[] { SV_HEAD_Y, SV_HEAD_P, SV_HEAD_R, SV_BODY_Y, SV_L_SHOULDER_P, SV_R_SHOULDER_P, SV_L_ELBOW_P, SV_R_ELBOW_P },
            new Short[] { HEAD_NEUTRAL_Y, HEAD_NEUTRAL_P, HEAD_NEUTRAL_R, BODY_NEUTRAL_Y, ARM_NEUTRAL, ARM_NEUTRAL, ARM_NEUTRAL, ARM_NEUTRAL },
            timeMs
        );
    }

    private void safePlayPose(Byte[] ids, Short[] vals, int timeMs) {
        try {
            CRobotPose pose = new CRobotPose();
            pose.SetPose(ids, vals);
            motion.play(pose, timeMs);
        } catch (Exception e) {
            servoErrorCount++;
            logWarn("play pose failed: " + e.getMessage());
        }
    }

    private short mapToServo(int value, int inMin, int inMax, short outMin, short outMax) {
        if (value < inMin) {
            value = inMin;
        }
        if (value > inMax) {
            value = inMax;
        }
        double ratio = (double)(value - inMin) / (double)(inMax - inMin);
        int mapped = (int)Math.round(outMin + ratio * (outMax - outMin));
        if (mapped < Math.min(outMin, outMax)) {
            mapped = Math.min(outMin, outMax);
        }
        if (mapped > Math.max(outMin, outMax)) {
            mapped = Math.max(outMin, outMax);
        }
        return (short)mapped;
    }

    private int randomRange(int min, int max) {
        return min + random.nextInt((max - min) + 1);
    }

    private String chooseConditionPhrase(RobotCondition c, Map<RobotCondition, String[]> bank) {
        String[] arr = bank.get(c);
        if (arr == null || arr.length == 0) {
            return "";
        }
        return arr[random.nextInt(arr.length)];
    }

    private String buildRecordingPath(int qIndex) {
        String day = new SimpleDateFormat("yyyyMMdd").format(new Date());
        String hms = new SimpleDateFormat("HHmmss").format(new Date());
        String cond = assignedCondition == null ? "UNASSIGNED" : assignedCondition.toString();
        return "recordings/" + participantId + "_" + day + "_" + cond + "_Q" + qIndex + "_" + hms + ".wav";
    }

    private long elapsedMillis() {
        return System.currentTimeMillis() - sessionStartEpochMs;
    }

    private String formatOffset(long ms) {
        long sec = ms / 1000L;
        long m = sec / 60L;
        long s = sec % 60L;
        return String.format(Locale.US, "%02d:%02d", m, s);
    }

    private int reflectInt(Object target, String[] methodNames, int fallback) {
        if (target == null) {
            return fallback;
        }

        for (int i = 0; i < methodNames.length; i++) {
            try {
                Method m = target.getClass().getMethod(methodNames[i], new Class<?>[0]);
                m.setAccessible(true);
                Object v = m.invoke(target, new Object[0]);
                if (v instanceof Number) {
                    return ((Number)v).intValue();
                }
            } catch (Exception e) {
                // try next
            }
        }
        return fallback;
    }

    private void joinThread(Thread t) {
        if (t == null) {
            return;
        }
        try {
            t.join(500);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
    }

    // ================================================================
    // main()
    // ================================================================

    public static void main(String[] args) {
        if (args.length < 1) {
            System.out.println("Usage: java jp.vstone.sotasample.DynamicVulnerabilityStudy <participant_id> [lang]");
            System.out.println("  participant_id example: P001");
            System.out.println("  lang: en (default) | ja");
            return;
        }

        String participantId = args[0];
        Language lang = Language.EN_US;
        if (args.length >= 2 && "ja".equalsIgnoreCase(args[1])) {
            lang = Language.JA_JP;
        }

        DynamicVulnerabilityStudy app = new DynamicVulnerabilityStudy(participantId, lang);

        if (!app.initialize()) {
            System.err.println("Initialization failed.");
            app.shutdown();
            return;
        }

        try {
            app.runMainLoop();
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            app.shutdown();
        }
    }
}
